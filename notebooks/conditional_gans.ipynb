{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f46cdfa-e07f-4580-9902-a48778f7afc1",
   "metadata": {},
   "source": [
    "# Conditional GANs (CGANs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3775f215-f5af-4128-9e59-47441b1ea2ad",
   "metadata": {},
   "source": [
    "__Objective:__ explore conditional GAN models.\n",
    "\n",
    "__Source:__ [notebook](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/blob/main/notebooks/04_gan/03_cgan/cgan.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc45f3b9-7ddc-41eb-b3d1-be2e3dd35d02",
   "metadata": {},
   "source": [
    "**Idea:** in usual GAN models, the generation process maps a randomly-sampled vector in latent space to a generated sample using the generator part of the model, while the discrimination process uses the discriminator part of the model to estimate the probability that a sample is taken from the real dataset rather than generated. Conditional GANs introduce additional inputs to the generation and discrimination processes, usually in the form of a class label, allowing e.g. to generate samples belonging to a specified class. In particular,\n",
    "- The generator takes (an encoding of) the class label as an additional input on top of the randomly-generated latent vector, and tries to convert the latent vector itself to a sample resembling those in the training dataset belonging to the class specified by the label. The generator tries to learn the conditional distribution $p(x | z, c)$, where $z$ is the latent vector and $c$ is the class label.\n",
    "- The discriminator also taks (an encoding of) a class label as an additional input, and tries to predict whether the provided sample comes from the real dataset **and belongs to the specified class**. The discriminator tries to lean the probability $p(\\text{real} | x, c)$, where $x$ is the sample and $c$ is again the class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13a8782-b643-423a-8dd2-9a040c7bc8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append('../modules/')\n",
    "\n",
    "from utils import preprocess_image\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f269cb9-f241-4dd8-be31-f8965214509b",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441ee292-4130-481b-bfe9-2eb06ab2a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/dataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d0628a-1221-4254-970b-7f03de3cf730",
   "metadata": {},
   "source": [
    "Infer labels from the images' filenames as returned by `os.walk` (see [documentation for Keras' `image_dataset_from_directory` function](https://keras.io/api/data_loading/image/)).\n",
    "\n",
    "**Labels choice:** we use the bricks dataset and split it into two classes, roof tiles (tiles with \"roof\" in their image name label `1`) and all the other tiles (label `0`). Not much imagination here, it's just an example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c74d9c-83d3-427c-b922-455c373529de",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = list(os.walk(DATA_DIR))[0][2]\n",
    "\n",
    "labels = [1 if ('roof' in name) else 0 for name in image_names]\n",
    "\n",
    "print(\n",
    "    'Fraction of samples with label 1:',\n",
    "    tf.constant(labels).numpy().mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7780d7a-e2c2-47c5-9081-ade0c4445555",
   "metadata": {},
   "source": [
    "Load the images with the specified labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61241a2-b713-4fb6-9b46-bdd2df27d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    labels=labels,\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=128,\n",
    "    image_size=(64, 64)\n",
    ")\n",
    "\n",
    "training_data = training_data.map(lambda img, label: (preprocess_image(img), tf.one_hot(label, depth=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b29284c-1d3f-49ad-a116-7511edfe1df9",
   "metadata": {},
   "source": [
    "Plot some random images belonging to both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962b4e7a-3522-4bb1-8752-99655a988ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_batch, labels_batch = next(iter(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcd0947-b817-499f-9292-3b5b377afafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 3\n",
    "\n",
    "images_plot = tf.stack(\n",
    "    [\n",
    "        images_batch[labels_batch[..., 0] == 0][:3, ...],\n",
    "        images_batch[labels_batch[..., 0] == 1][:3, ...]\n",
    "    ],\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=n_images, figsize=(14, 6))\n",
    "\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(n_images):\n",
    "        ax = axs[i, j]\n",
    "\n",
    "        ax.imshow(\n",
    "            images_plot[i, j, ...],\n",
    "            cmap='gray'\n",
    "        )\n",
    "\n",
    "        ax.grid(False)\n",
    "\n",
    "        plt.sca(ax)\n",
    "        plt.title(f'Label: {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3460a4db-65a0-44bd-9592-7034ffe0a487",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c059a0-c10c-441e-9f03-0d72a099f1cf",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47217e39-8322-4faf-8441-e4f33230ea31",
   "metadata": {},
   "source": [
    "The generator is a simple adaptation of the usual GAN/WGAN generator model, this time accepting the class label as an additional input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74120424-02ab-4ae6-9368-0cc61810c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import CGANGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f5183d-c8c7-4747-b181-5187e63edbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = CGANGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9692aa7a-55ab-4e2e-b6cd-84c5c4751820",
   "metadata": {},
   "source": [
    "The generator's input for CGANs is a list with the first element being the randomly-generated latent vector and the second element being the one-hot encoded class labels (these tensors will be concatenated as the first step in the generation process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4044163-df10-4309-9d19-2d1c96c9bbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the generator.\n",
    "generator([tf.random.normal(shape=(1, 100)), labels_batch[:1, ...]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff9a8b5-e857-49f9-8c24-b30a1dcdaa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b9e97b-264b-4614-be9a-f8c1d9ec820c",
   "metadata": {},
   "source": [
    "### Discriminator (critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825043c0-99b8-4880-8de0-ff11a3f762b0",
   "metadata": {},
   "source": [
    "The discriminator is also an adaptation of the GAN/WGAN one, again accepting the class label as an additional input. Because images are higher-rank tensors w.r.t. the one-hot encoded class labels, their concatenation is not as straightforward as in the generator: indeed, the class label tensors' dimensions are expanded and the values repeated along the new axes (see the implementation in the module for more details). This reshaping happens **before** everything is passed to the critic and is implemented as a static method of the full model class (`ConditionalWGANGP`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5f85a-0dce-47b7-8028-75abd483f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wgan_gp_critic import CGANCritic\n",
    "from wgan_gp import ConditionalWGANGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0850b2c9-6f5d-4fb7-8c10-0e400c0aeaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = CGANCritic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85f9d9b-92ad-4032-941c-c42bd112f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConditionalWGANGP.expand_label_tensor(labels_batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee98a380-5add-4bbc-a8c5-6a76eeb49e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction.\n",
    "critic([images_batch[:1, ...], ConditionalWGANGP.expand_label_tensor(labels_batch[:1, ...])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65439c5-761e-484e-aa2e-c09a5e27ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aff5b6d-77be-426a-b5dd-d74e14c75bf2",
   "metadata": {},
   "source": [
    "### Full conditional WGAN-GP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b8cc69-d67a-403a-846d-05fae212e1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwgan_gp_model = ConditionalWGANGP(\n",
    "    critic=critic,\n",
    "    generator=generator,\n",
    "    latent_dim=100,\n",
    "    critic_steps=3,\n",
    "    gp_weight=10.\n",
    ")\n",
    "\n",
    "cwgan_gp_model.compile(\n",
    "    c_optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "    g_optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9460d294-6165-4460-b1fe-2cb1a2c147c8",
   "metadata": {},
   "source": [
    "Test a training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d54e97-5709-427f-a525-80d6cefe2b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwgan_gp_model.train_step([images_batch, labels_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c9c5a-dfa8-4d6b-9700-c7fb733e09e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwgan_gp_model.fit(\n",
    "    training_data,\n",
    "    epochs=1,\n",
    "    steps_per_epoch=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
